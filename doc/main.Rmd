---
title: "Project 4 - OCR (Optical Character Recognition)"
author: "Section 1 - Group 8"
output:
  html_document:
    df_print: paged
---


# Step 1 - Load library and source code
```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("stringdist")) install.packages("stringdist")
if (!require("dplyr")) install.packages("dplyr")
if (!require("gbm")) install.packages("gbm")
if (!require("ada")) install.packages("ada")
if (!require("ebmc")) install.packages("ebmc")
if (!require("data.table")) install.packages("data.table")

if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}

library(stringdist)
library(stringr)
library(dplyr)
library(gbm)
library(ada)
library(ebmc)
library(data.table)

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - read the files and conduct Tesseract OCR

Although we have processed the Tesseract OCR and save the output txt files in the `data` folder, we include this chunk of code in order to make clear the whole pipeline to you.

```{r, eval=FALSE}
for(i in c(1:length(file_name_vec))){
  current_file_name <- sub(".txt","",file_name_vec[i])
  ## png folder is not provided on github (the code is only on demonstration purpose)
  current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
  
  ### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
  clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
  clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
  
  ### save tesseract text file
  writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
```

# Step 3 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words* -- check to see if an input string is a valid dictionary word or if its n-grams are all legal.

Paper: C2

Here we followed the paper and implemented the rule-based methodology.
```{r, warning=FALSE, message = FALSE}
rerun_data = F
source("../lib/line_mismatch_finder.R")
source("../lib/d1_detect.R")
source("../lib/remove_empty_line.R")

total_ground_truth_txt_100 <- list()
total_tesseract_txt_100 <- list()
total_mismatch_info_100 <- list()
total_d1_output <- list()
total_detect_output <- list()
total_tesseract_clean_output <- list()
total_mismatch_rownum_100 <- matrix(NA, nrow = length(file_name_vec), ncol = 2)


for (k in 1:length(file_name_vec)){
  current_file_name <- sub(".txt","",file_name_vec[k])
  
  ## read the ground truth text
  current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
  ## read the tesseract text
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
  #clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
  
  ## Remove punctuations & remove leading and trailing zeros
  current_ground_truth_txt <- gsub('[[:punct:]]+','',current_ground_truth_txt)
  current_ground_truth_txt <- trimws(current_ground_truth_txt, which = "both")
  current_tesseract_txt <- gsub('[[:punct:]]+','',current_tesseract_txt)
  current_tesseract_txt <- trimws(current_tesseract_txt, which = "both")
  
  ## Record mismatching lines
  mismatch_info <- line_mismatch_finder(tess = current_tesseract_txt, grdth = current_ground_truth_txt)
  
  ## Save into List
  total_tesseract_txt_100[[k]] <- current_tesseract_txt
  total_ground_truth_txt_100[[k]] <- current_ground_truth_txt
  total_mismatch_info_100[[k]] <- mismatch_info
}
  
  ################################################
  ### Please note that we figured there are exactly 13 text files whose total number of lines do not match 
  ### between their corresponding ground_truth and tesseract files. Therefore, we manually trimmed the
  ### lines of those ground_truth files
  ################################################
  
  ## Check if there are any mismatches in terms of article's total number of rows
  for (k in 1:length(file_name_vec)){
    total_mismatch_rownum_100[k,1] <- length(total_tesseract_txt_100[[k]])
    total_mismatch_rownum_100[k,2] <- length(total_ground_truth_txt_100[[k]])
  }
  ## It turns out all articles match now
  sum(total_mismatch_rownum_100[,1]==total_mismatch_rownum_100[,2]) == 100

  ## Code to locate the index of the corresponding ground truth text
  ## 1. if the number of words in corresponding row (between tesseract and ground_truth) are equal,
  ## then locate the ground truth word by indexing directly
  ## 2. if the number of words in corresponding row are not equal, extract previous and following 2 
  ## words of the error word (total of 5 index), and apply string-distance function (stringdist) 
  ## to locate the most likely ground truth word.
  
  for (k in 1:length(file_name_vec)){
    current_d1_output <- d1_detect(current_tesseract_txt = total_tesseract_txt_100[[k]],
                                        current_ground_truth_txt = total_ground_truth_txt_100[[k]],
                                        mismatch_info = total_mismatch_info_100[[k]])
    
    total_detect_output[[k]] <- remove_empty_line(current_d1_output[[1]]) ## remove any lines with empty tess_err or empty ground_truth_err
    total_tesseract_clean_output[[k]] <- current_d1_output[[2]]
    #print(k)  ## For debug purpose only
  }


  if (rerun_data){
    save(total_tesseract_txt_100, file="../output/total_tesseract_txt_100.RData")
    save(total_ground_truth_txt_100, file="../output/total_ground_truth_txt_100.RData")
    save(total_detect_output, file="../output/total_detect_output.RData")
    save(total_tesseract_clean_output, file="../output/total_tesseract_clean_output.RData")
  }



```

# Step 4 - Error correction

Paper: C2

Load data from step 3
```{r}
load("../output/total_tesseract_txt_100.RData")         ## combined tesseract_txt from all 100 articles
load("../output/total_ground_truth_txt_100.RData")      ## combined ground_truth_txt from all 100 articles
load("../output/total_detect_output.RData")             ## detected error words with their suspected ground truth words from all 100 articles
load("../output/total_tesseract_clean_output.RData")    ## combined tesseract_clean_txt from all 100 articles
```


Construction corpus and look for top candidates
By far, we only random sampled 5 papers for further analysis (because of time constraint)
```{r}
set.seed(2018)
num <- 100       #input the number of articles used
selected_index <- sample(1:100,num)

random_list <- function(input_list, selected_index = selected_index){
  temp <- c()
  
  for(i in selected_index){
    temp <- c(temp,input_list[[i]])
  }
  return(list(temp))
}

current_ground_truth_txt <- random_list(total_ground_truth_txt_100, selected_index)
current_tesseract_txt <- random_list(total_tesseract_txt_100, selected_index)
current_tesseract_clean_txt <- random_list(total_tesseract_clean_output, selected_index)

corpus <- c()
for(i in 1:length(strsplit(current_ground_truth_txt[[1]],split = ' '))){
  corpus <- c(corpus,strsplit(current_ground_truth_txt[[1]],split = ' ')[[i]])
}

error_words <- c()
for(i in selected_index){
  error_words <- c(error_words,as.character(total_detect_output[[i]][,1]))
}

corpus_matrix <- matrix(unique(corpus),nrow = length(unique(corpus)),ncol = 1)

distance <- function(x){
  return(stringdist::stringdist(x,word))
}

total_candidate <- list()
candidate_time<-system.time(
for(i in error_words){
  word <- i
  result <- apply(corpus_matrix,1,distance)
  index <- order(result)[1:50]
  total_candidate[[word]] <- corpus_matrix[index,]
}
)
#save(total_candidate, file="../output/total_candidate.RData")
#load("../output/total_candidate.RData")
```

Construct training set
```{r, warning=FALSE, message = FALSE}
source("../lib/feature_score.R")
training <- matrix(NA,nrow = sum(lengths(total_candidate)),ncol = 6)

comparison <- data.frame(tesseract_err = 0,ground_truth_err = 0)
comparison <- comparison[-1,]
for(i in selected_index){
  comparison <- rbind(comparison,total_detect_output[[i]])
}

trainset_time<-system.time(
  for(i in 1:length(total_candidate)){
    can_matrix <- matrix(total_candidate[[i]],ncol = 1)
    w_e <- names(total_candidate)[i]
    true_correction <- filter(comparison,tesseract_err == w_e)[,2]
    
    partframe <- data.frame(rep(w_e,lengths(total_candidate)[i]),total_candidate[[i]],
                            apply(can_matrix,1,LED_score),apply(can_matrix,1,SS_score),
                            apply(can_matrix,1,LP_score),apply(can_matrix,1,correction))
    partframe <- as.matrix(partframe)
    training[(sum(lengths(total_candidate)[1:i])-lengths(total_candidate)[i]+1)
             :sum(lengths(total_candidate)[1:i]),] <- partframe
  }
)
training <- as.data.frame(training)
colnames(training) <- c('error','candidate','score1','score2','score3','correction')

training$correction <- ifelse(training$correction=='TRUE'|training$correction==' TRUE','1','0')
#training
```

Train model
```{r}
### handle imbalanced data
training_one <- filter(training,correction == '1')
training_zero <- filter(training,correction == '0')
training_sub <- training_zero[sample(1:nrow(training_zero),nrow(training_one)),]
training_balance <- rbind(training_one,training_sub)
```

Prediction
```{r}
#model <- gbm.fit(x = training_balance[,c(3,4,5)],y = training_balance[,6],distribution = 'adaboost',n.trees = 100)
#prediction <- predict(model,training[,c(3,4,5)],n.trees = gbm.perf(model))

#model <- gbm(correction ~ score1 + score2 + score3,data = training_balance,distribution = 'adaboost',n.trees = 2000)
#prediction <- predict(model,training[,c(3,4,5)],n.trees = 2000)

#model <- ada(x = training_balance[,c(3,4,5)],y = training_balance$correction,loss = 'logistic')
#predict(model,training[,c(3,4,5)])

model <- adam2(correction ~ score1 + score2 + score3,data = training_balance,size = 100,alg = 'cart')
prediction<-predict(model,training[,c(3,4,5)])
```

Select top 3 candidates for each word
```{r}
training$confidence <- prediction
final_result <- list()
for(i in unique(training$error)){
  data <- filter(training,error == i)
  index <- order(data$confidence,decreasing = T)[1:3]
  top_candidate <- data$candidate[index]
  final_result[[i]] <- top_candidate
}
#final_result
```

Construct prediction dataframe for further analysis
```{r, warning=FALSE, message = FALSE}

df = data.frame(final_result, row.names = NULL)
df = t(df)
df = data.frame(df)
df <- add_rownames(df, "tesseract_err")
colnames(df) = c("tesseract_err", "Top1", "Top2", "Top3")

comparison1 = as.data.frame(comparison,stringsAsFactors=F)
final = merge(df, comparison1, by = "tesseract_err", all.y=TRUE)
final = na.omit(final)
#final$index <- ifelse()

final <- data.frame(lapply(final, as.character), stringsAsFactors=FALSE)
for (i in 1:nrow(final)){
  final$index[i] <- ifelse(length(which(final$ground_truth_err[i] == final[i,2:4]))>0, which(final$ground_truth_err[i] == final[i,2:4]),0)
}


```


# Step 5 - Performance measure

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

Both *precision* and *recall* are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.

Here, we only finished the **word level evaluation** criterions, you are required to complete the **letter-level** part.

```{r}
target_char_only <- c(letters,0,1,2,3,4,5,6,7,8,9)

## Load word vectors
ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]]
tesseract_vec <- str_split(paste(current_tesseract_txt, collapse = " ")," ")[[1]] 
tesseract_delete_error_vec <- current_tesseract_clean_txt[[1]]
#predict_vec <- final[final$index!=0,5]
predict_vec <- final[final$index==1,5]
predict_grdth_vec <- final[,5]

## Remove punctuations from word vectors
ground_truth_vec_remove <- gsub('[[:punct:]]+','',ground_truth_vec)
tesseract_vec_remove <- gsub('[[:punct:]]+','',tesseract_vec)
tesseract_delete_error_vec_remove <- gsub('[[:punct:]]+','',tesseract_delete_error_vec)
predict_vec_remove <- gsub('[[:punct:]]+','',predict_vec)
predict_grdth_vec_remove <- gsub('[[:punct:]]+','',predict_grdth_vec)

## Word Level evaluation
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec_remove), tolower(tesseract_vec_remove)) 
new_intersect_vec_1 <- vecsets::vintersect(tolower(ground_truth_vec_remove), tolower(tesseract_delete_error_vec_remove)) 
new_intersect_vec_2 <- vecsets::vintersect(tolower(predict_grdth_vec_remove), tolower(predict_vec_remove))
new_intersect_vec <- c(new_intersect_vec_1, new_intersect_vec_2)

## Character Level evaluation
ground_truth_cha <- unlist(strsplit(tolower(ground_truth_vec),split = ""))
ground_truth_table_all_cha <- length(ground_truth_cha[ground_truth_cha %in% target_char_only])

tesseract_cha<- unlist(strsplit(tolower(tesseract_vec),split = ""))
tesseract_cha_table_all_cha <- length(tesseract_cha[tesseract_cha %in% target_char_only])

common_cha <- vecsets::vintersect(unlist(strsplit(tolower(ground_truth_vec),"")), unlist(strsplit(tolower(tesseract_delete_error_vec),"")))
common_cha_table_all_cha <- length(common_cha[common_cha %in% target_char_only])

common_cha_post <- vecsets::vintersect(unlist(strsplit(tolower(predict_grdth_vec),"")), unlist(strsplit(tolower(predict_vec),"")))
common_cha_table_post <- length(common_cha_post[common_cha_post %in% target_char_only])

## Create table
OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",
                                                 "character_wise_recall","character_wise_precision")
OCR_performance_table["word_wise_recall","Tesseract"] <- length(old_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract"] <- length(old_intersect_vec)/length(tesseract_vec)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(tesseract_delete_error_vec)
OCR_performance_table["character_wise_recall","Tesseract"] <- 
  common_cha_table_all_cha/ground_truth_table_all_cha
OCR_performance_table["character_wise_precision","Tesseract"] <- 
  common_cha_table_all_cha/tesseract_cha_table_all_cha
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- 
  (common_cha_table_post+common_cha_table_all_cha)/ground_truth_table_all_cha
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- 
  (common_cha_table_post+common_cha_table_all_cha)/tesseract_cha_table_all_cha
kable(OCR_performance_table, caption="Summary of OCR performance")

#OCR_performance_table
```
